---
title: "Optimization problems in support vector machines"
author: "Pol Barrachina, Kevin Michael Frick"
bibliography: lab2_svm.bib
output:
  pdf_document
---

# Support vector machines

The assignment is concerned with the analysis of the optimization problems entailed by the usage of a support vector machine as a classification algorithm. 

The primal formulation of the SVM is implemented in AMPL as follows. 

```{r svmprimal, echo=FALSE, results="markup", comment=""}
cat(readLines("SVM.mod"),sep="\n")
```
The dual, on the other hand, is implemented as follows. 
```{r svmdual, echo=FALSE, results="markup", comment=""}
cat(readLines("DualSVM.mod"),sep="\n")
```

In the dual model, the `.run` file is slightly more complex because the primal solution must be retrieved from the dual model. 

Given the dual solution vector $\mathbf{\lambda}$, the primal solution $\mathbf{w}$ is found as
$w_j = \sum_{i=1}^m \lambda_i y_i A_{ij}$.

The plane intercept $\gamma$, on the other hand, is found exploiting complementary slackness.
Since $\lambda_i (y_i (\mathbf{w}^T \mathbf{x}_i + \gamma) - 1 + \zeta_i) = 0$, when a point $k$ is a non-bound support vector we have $\lambda_k \in (0, \nu) \implies \zeta_k = 0$ and therefore $\gamma = \frac{1}{y_k} - \mathbf{w}^T \mathbf{x}_i$. 

Since the solution is found numerically, the condition $\lambda_k \in (0, \nu)$ is checked with some slack $\epsilon$ to counter numerical errors in the computation.

This results in the following AMPL code.

```{r svmdualrun, echo=FALSE, results="markup", comment=""}
cat(readLines("DualSVM.run"),sep="\n")
```

# Computational study

## gensvmdat

We now run the primal and dual formulation of the SVM on the same dataset, generated with the provided `gensvmdat` program, then retrieve the primal solution from the dual as$w_j = \sum_{i=1}^m \lambda_i y_i A_{ij}$.
We will then check that the primal and dual solutions are the same.

```{bash, engine.opts='-l', echo=FALSE}
AMPL_PATH="/Applications/ampl_macos64"
NUM_PARAM=4
TR_SEED=50321
TE_SEED=12305
TRAIN_FNAME="train_50.dat"
INPUT_FNAME="train.dat"
NU=0.5
# Remove asterisks
sed -i ''  -e 's/\*//g'  ${TRAIN_FNAME}
cp ${TRAIN_FNAME} ${INPUT_FNAME}
export PATH=${PATH}:${AMPL_PATH}
m=50
echo "m = ${m}, nu = ${NU}, Primal SVM:"
printf "${NUM_PARAM}\n${m}\n${NU}" | ./SVM.run | tail -n $(expr ${NUM_PARAM} + 1) |  tee out_${TRAIN_FNAME}
echo "m = ${m}, nu = ${NU}, Dual SVM:"
printf "${NUM_PARAM}\n${m}\n${NU}" | ./DualSVM.run | tail -n ${NUM_PARAM}
rm ${INPUT_FNAME}
```

We can see that both the primal and dual SVM obtain the same parameter vector $\mathbf{w}$. 
This is expected because the SVM, as a quadratic problem, benefits from strong duality, which means that the primal and dual solutions will be the same. 

We will now test that this holds on a bigger dataset. Moreover, we will test the accuracy of the obtained parameter vector on a test set that is different from the training set. 

```{bash, engine.opts='-l', echo=FALSE}
AMPL_PATH="/Applications/ampl_macos64"
PATH=${PATH}:${AMPL_PATH}
NUM_PARAM=4
TR_SEED=50321
TE_SEED=12305
TRAIN_FNAME="train_10000.dat"
INPUT_FNAME="train.dat"
NU=0.5
NUM_DATA_PT=10000
cp ${TRAIN_FNAME} ${INPUT_FNAME}
# Remove asterisks
sed -i ''  -e 's/\*//g' ${INPUT_FNAME}
echo "m = ${NUM_DATA_PT}, nu = ${NU}, Primal SVM:"
echo -e "${NUM_PARAM}\n${NUM_DATA_PT}\n${NU}" | ./SVM.run | tail -n ${NUM_PARAM}
echo "m = ${NUM_DATA_PT}, nu = ${NU}, Dual SVM:"
#echo -e "${NUM_PARAM}\n${NUM_DATA_PT}\n${NU}" | ./DualSVM.run | tail -n ${NUM_PARAM} 
rm ${INPUT_FNAME}
```

Again, the parameter vectors which result from solving the primal and dual problems are the same. 
In order to classify a point, we now only need to see on which side of the hyperplane does it fall, that is if $\mathbf{w}^T \mathbf{x} + \gamma > 0$. 

```{r solvesvmfunc, echo=FALSE, results="markup", comment=""}
parse_ampl_out <- function(param_file_name) {
  w = c()
  con = file(param_file_name, "r")
  gamma = as.numeric(strsplit(readLines(con, n = 1), " ")[[1]][3])
    while (TRUE) {
      line = readLines(con, n = 1)
      if (length(line) == 0) {
        break
      }
      w = unlist(c(w, as.numeric(strsplit(line, " ")[[1]][3])))
    }
  params <- list("w" = w, "gamma" = gamma)
}

get_svm_accuracy <- function(data_file_name, w, gamma, num_param) {
  data <- read.table(data_file_name)
  nts <- 0
  ncor <- 0
  for (i in 1:nrow(data)) {
    y <- t(w) %*% as.numeric(data[i, 1:num_param]) + gamma > 0
    nts <- nts + 1
    if ((y == TRUE &&  data[i, num_param+1] == 1) || (y == FALSE &&  data[i, num_param+1] != 1)) {
      ncor <- ncor + 1
    }
  }
  acc <- c(ncor, nts)
}
```

```{r solvesvm, echo=FALSE, results="markup", comment=""}
PARAM_FILE_NAME = "out_train_50.dat"
DATA_FILE_NAME = "train_50.dat"
params <- parse_ampl_out(PARAM_FILE_NAME)
w = params$w
gamma = params$gamma
cat ("w = ", w, "\n")
cat("gamma = ", gamma, "\n")

svm_acc <- get_svm_accuracy(DATA_FILE_NAME, w, gamma, 4)
ncor <- svm_acc[1]
nts <- svm_acc[2]
cat("Correctly classified", ncor, "/", nts, " points on the training set", "\n")
```

We can see that, on a dataset of 50 points, the trained SVM is able to correctly classify 9499 of the, or about 95%.

## Rice Varieties

We will now test the primal and dual formulations on a dataset concerning different kinds of rice, in order to ensure that, as expected, primal and dual formulations of the SVM give the same results.

```{bash, engine.opts='-l', echo=FALSE}
AMPL_PATH="/Applications/ampl_macos64"
NUM_PARAM=7
TRAIN_FNAME="rice.dat"
INPUT_FNAME="train.dat"
DUAL_OUT_FNAME="dual_out.txt"
NU=0.5
# Remove asterisks
sed -i ''  -e 's/\*//g'  ${TRAIN_FNAME}
cp ${TRAIN_FNAME} ${INPUT_FNAME}    
export PATH=${PATH}:${AMPL_PATH}
m=3809
echo "m = ${m}, nu = ${NU}, Primal SVM:"
time $(printf "${NUM_PARAM}\n${m}\n${NU}" | ./SVM.run | tail -n $(expr ${NUM_PARAM} + 1) > out_${TRAIN_FNAME})
cat out_$TRAIN_FNAME
echo "m = ${m}, nu = ${NU}, Dual SVM:"
time $(printf "${NUM_PARAM}\n${m}\n${NU}" | time ./DualSVM.run | tail -n ${NUM_PARAM} > ${DUAL_OUT_FNAME})
cat ${DUAL_OUT_FNAME}
rm ${DUAL_OUT_FNAME}
rm ${INPUT_FNAME}
```


```{bash, engine.opts='-l', echo=FALSE}
AMPL_PATH="/Applications/ampl_macos64"
PATH=${PATH}:${AMPL_PATH}
NUM_PARAM=7
INPUT_FNAME="train.dat"
TRAIN_FNAME="rice.dat"
NUM_DATA_PT=3809
cp ${TRAIN_FNAME} ${INPUT_FNAME}
for NU in 0.1 0.5 1 5
do
  echo -e "${NUM_PARAM}\n${NUM_DATA_PT}\n${NU}" | ./SVM.run | tail -n $(expr ${NUM_PARAM} + 1) |  tee out_nu${NU}_${TRAIN_FNAME}
done
rm ${INPUT_FNAME}
```
  
```{r testsvmrice, echo=FALSE, results="markup", comment=""}

for (nu in c(0.1, 0.5, 1, 5)) {
  paramfile <- paste("out_nu", nu, "_rice.dat", sep="")
  print(paramfile)
  params <- parse_ampl_out(paramfile)
  w = params$w
  gamma = params$gamma
  cat ("w = ", w, "\n")
  cat("gamma = ", gamma, "\n")
  
  datafile <- "rice.dat"
  svm_acc <- get_svm_accuracy(datafile, w, gamma, 7)
  ncor <- svm_acc[1]
  nts <- svm_acc[2]
  cat("Correctly classified", ncor, "/", nts, " points", "\n")
}
```

# Computational study

After generating a .csv file with the outputs resulting from different datasets, varying the values of $\nu$, we will now observe variations in accuracy and CPU time across different formulations.  

```{r readdata, echo=FALSE}
data <- read.csv('test.csv', header=TRUE, sep=";")
data$nu <- as.factor(data$nu)
divide_string <- function(str) {
 str <- strsplit(str, ",")[[1]]
 round(as.numeric(str[1]) / as.numeric(str[2]), 3)
}
data$acc <- unlist(lapply(data$acc, divide_string))
```

First, we plot CPU time with respect to model formulation, primal and dual.

```{r cputime, echo = FALSE}
boxplot(cpusec ~ model, data = data, xlab = "Model", ylab="CPU time (s)", col = 10:15)
```

We can see that the dual formulation is much slower in general. This is expected because, on both datasets, $n >> d$ and the dual formulation is nonparametric, i.e. scales with dataset size and not parameter count. 

We now plot accuracy with respect to $\nu$.
```{r acc, echo = FALSE}
boxplot(acc ~ nu, data = data, xlab = "nu", ylab="Accuracy", col = 10:15)
```

We can see that accuracy increases with $\nu$, topping out at $\nu = 1$. 
This is expected because the $\nu$ parameter can be interpreter as an inverse regularizer of the solution, which increases goodness of fit on the training set. 
The limit value of $\nu = \infty$ translates to the hard-margin SVM, where separation is perfect.

```{r acc, echo = FALSE}
library(dplyr)
boxplot(acc ~ sigmasq, data = filter(data, sigmasq != 'n/a'), xlab = "Sigma^2", ylab="Accuracy", col = 10:15)
```

# Conclusions
