---
title: "Optimal and heuristic solutions for the cluster median problem"
author: |
  | Pol Barrachina, Kevin Michael Frick
  | Facultat d'Inform√†tica de Barcelona
bibliography: lab3_clustermedianproblem.bib
date: "January 2022"
output:
  pdf_document:
    fig_caption: yes
    highlight: pygments
    keep_tex: yes
    number_sections: yes
    toc: yes
header-includes: 
  - \usepackage{float}
  - \usepackage{longtable}
  - \usepackage{amsmath}
---
\centering

\raggedright

\clearpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(caret)
library(GGally)
library(network)
library(sna)
library(ggplot2)
```

<!-- 
1. DONE A cover page with the name of the two (and exactly two!) members of the group
2. DONE A description of the data matrix A.
3. DONE The AMPL .mod and .dat files.
4. DONE The optimal solution obtained with AMPL.
5. DONE A description of how the minimum spanning tree was computed (which software used,
etc.)
6. DONE The heuristic solution obtained with the minimum spanning tree procedure.
7. DONE A comparison of the two solutions obtained, in terms of the objective function of the
integer optimization problem. Optionally, you may provide any other additional criteria
for comparison.
8. Any other observation or comment you may want to add, or any problem you had when
performing the assignment.
-->

# The cluster median problem 

The cluster-median problem is a particular problem in cluster analysis, or clustering, a method of unsupervised learning.

Given a data matrix $A \in \mathbb{R}^{m \times n}$ of $m$ points and $n$ variables, the
goal is to group them in $k$ clusters, where $k$ is a hyperparameter.
A criterion for clustering has to be defined, and in the cluster-median problem this translates into requiring that the overall distance of all the points to the median of their cluster to be minimized.
The median for a subset of points $I \subseteq \{1, . . . , m\}$ is defined as the nearest point to all points
of $I$, that is $r$ is the median of $I$ if $\sum_{i \in I} d_{ir} = \min_{j \in I} \sum_{i \in I} d_{ij}$

where $D \in \mathbb{R}^{m \times n}$ is the distance matrix between points. We will use Euclidean distance in this assignment.

The cluster-median problem can be formulated as an integer optimization problem.
This provides an acceptable solution to the cluster-median problem, but is considered NP-hard in general, so it is only a viable for small data matrices.

# Description of the dataset

We use a set of data on geometric parameters of three different varieties of wheat (@charytanowicz2010complete).

The parameters are:

1. area $A$
2. perimeter $P$
3. compactness $C = 4\pi \frac{A}{P^2}$
4. length of kernel
5. width of kernel
6. asymmetry coefficient
7. length of kernel groove

All of these parameters are real-valued and continuous. We remove parameter 3 because it is a function of parameters 1 and 2. There are 70 observation per type of wheat.

The $A$ matrix has therefore a final size of $210 \times 6$.

We read the data file, remove the third and last column, as the last column corresponds to the correct cluster, then compute the distance matrix and write it to a file to be read by AMPL. We also perform principal component analysis to plot the dataset.

```{r}
seeds <- read.table('seeds.txt')
seeds.clean <- seeds[,-c(3, 8)]
seeds.dist <- as.matrix(dist(seeds.clean))
write.table(seeds.dist, file='seeds_dist.dat', row.names = FALSE, col.names = FALSE)
seeds.pca <- princomp(seeds.clean)
plot(seeds.pca$scores[,1:2])
```

# Modeling the CMP in AMPL

We formulate the integer optimization problem as follows:

\begin{equation}
\begin{array}{l l l}
\min & \sum\limits_{i = 1}^m \sum\limits_{j = 0}^m d_{ij} x_{ij} &\\
s.t. & \sum\limits_{j = 1}^m x_{ij} = 1 & \forall \; i \in \{1..m\}\\
& \sum\limits_{j = 1}^m x_{jj} = k &\\
& x_{jj} \geq x_{ij} & \forall \; i,j \in \{1..m\} \\
& x_{ij} \in \{0, 1\}&

\end{array}
\end{equation}

The first constraint means that every point must belong to one cluster.
The second requires that there be exactly $k$ clusters.
The third means that A point may belong to a cluster only if the cluster exists.
The last constraints means the variables are binary.

This results in the following AMPL `.mod` file:

```
param m := 210; # 70 data points per variety
param n := 6; # 6 features
param k := 3; # 3 varieties of wheat
param d {i in 1..m, j in 1..m};
var x {i in 1..m, j in 1..m} binary;

minimize cluster: sum{i in 1..m}(sum{j in 1..m} d[i,j] * x[i,j]);

subject to onecluster {i in 1..m}:
sum{j in 1..m} x[i,j] = 1;

subject to kclusters:
sum{j in 1..m} x[j,j] = k;

subject to clusterexists {i in 1..m, j in 1..m}:
x[j,j] >= x[i,j];
```

Which we call using the following `.run` file:
```
reset;
model cluster.mod;
for {i in 1..m} {
	for {j in 1..m} {
		read d[i, j] < seeds_dist.dat;
	}
}
option solver cplex;
solve;
for {i in 1..m} {
	for {j in 1..m} {
		printf "%d ", x[i, j] > seeds_out.dat;
	}
	printf "\n" > seeds_out.dat;
}
```

Which gives the following output:
```
CPLEX 20.1.0.0: optimal integer solution; objective 314.2081702
11 MIP simplex iterations
0 branch-and-bound nodes
```

After running the file, we check which points correspond to the cluster medians:
```{r}
get.cluster.medians <- function(outf) {
  ampl.cluster.medians <- c()
  for (i in 1:nrow(outf)) { 
    if (any(outf[,i] != 0)) { 
      ampl.cluster.medians <- c(ampl.cluster.medians, i)
    } 
  }
  ampl.cluster.medians
}
seeds.out <- read.table('seeds_out.dat')
seeds.ampl.cluster.medians <- get.cluster.medians(seeds.out)
cat("Cluster medians are: ", seeds.ampl.cluster.medians, "\n")
```

We check which varieties do those points belong to in the original dataset:

```{r}
print(seeds[seeds.ampl.cluster.medians,])
```

Since we have ground truth labels, we can print the confusion matrix.

```{r}
get.confusion.matrix <- function(true.clust, out, k) {
  predicted.clust <- as.data.frame(matrix(0, nrow = nrow(out), ncol = 1))
  ampl.cluster.medians <- get.cluster.medians(out)
  i <- 1
  for (col in out[,ampl.cluster.medians]) {
    clust.i <- which(col == 1)
    predicted.clust[clust.i,1] <- i
    i <- i + 1
  }
  predicted.clust
  confusionMatrix(as.factor(predicted.clust[,1]), true.clust)
}
k <- 3
seeds.confusion.matrix <- get.confusion.matrix(as.factor(seeds[,8]), seeds.out, k)
print(seeds.confusion.matrix)
```

The value of the objective function is:

```{r}
dx <- seeds.out * seeds.dist
cat("sum(dij * xij) = ", sum(dx))
```

Which matches what is given by AMPL.

# The CMP as a MST problem

We implement a union-find disjoint set data structure (UFDS). We use path compression to keep the tree short and always attach in such a way as to keep the tree short, using the rank of a node to choose which attachment point to use.

```{r}
reset.ufds <- function(m) {
  ufds.rank <<- rep(0, m)
  p <<- as.numeric(1:m)
}
find.set <- function (i) {
  if (p[i] == i) {
    return(i)
  } else {
    # Optimization: path compression
    return(p[i] <<- find.set(p[i]))
  }
}
is.same.set <- function(i, j) {
  set.i <- find.set(i)
  set.j <- find.set(j)
  return(set.i == set.j)
}
union.set <- function(i, j) {
  if (!is.same.set(i, j)) {
    set.i <- find.set(i)
    set.j <- find.set(j)
    # Optimization: use rank to keep the tree short
    if (ufds.rank[set.i] > ufds.rank[set.j]) {
      p[set.i] <<- set.j
    } else {
      p[set.j] <<- set.i
    }
    if (ufds.rank[set.i] == ufds.rank[set.j]) {
      ufds.rank[set.j] <<- ufds.rank[set.j] + 1
    }
  }
}
```

We implement Kruskal's algorithm to get the minimum spanning tree.

```{r}
gen.edge.list <- function(datamat) {
  dist.matrix <- as.matrix(dist(datamat))
  m <- nrow(dist.matrix)
  edge.list <- as.data.frame(matrix(0, nrow = m * m - m, ncol = 3))
  names(edge.list) <- c('from', 'to', 'dist')
  cnt <- 1
  for (i in 1:m) {
    for (j in 1:m) {
      if (j != i) {
        edge.list[cnt,] <- c(i, j, dist.matrix[i, j])
        cnt <- cnt + 1
      }
    }
  }
  edge.list
}
seeds.edge.list <- gen.edge.list(seeds.clean)
```

```{r}
kruskal <- function(edge.list, m) {
  mst.cost <- 0
  reset.ufds(m)
  edge.list.ordered <- edge.list[order(edge.list$dist), ]
  mst.edge.list <- data.frame(row.names = names(edge.list))
  for (i in 1:nrow(edge.list)) {
    u <- edge.list.ordered[i,]
    if (!is.same.set(u$to, u$from)) {
      mst.cost <- mst.cost + u$dist
      union.set(u$to, u$from)
      mst.edge.list <- rbind(mst.edge.list, u)
    } 
  }
  cat("MST cost = ", mst.cost)
  mst.edge.list
}
m <- nrow(seeds.dist)
seeds.mst.edge.list <- kruskal(seeds.edge.list, m)
```

Now, we remove the $k - 1 = 2$ edges with largest weight in order to get three separate trees, which will be our clusters.

```{r}
k <- 3
n.edges <- nrow(seeds.mst.edge.list)
seeds.clust.mst.edge.list <- seeds.mst.edge.list[1:(n.edges - (k - 1)),]
```

We now run a connected components search to color the nodes according to their cluster.

```{r}
gen.dist.mst <- function(clust.edge.list, m) {
  dist.mst <- matrix(0, nrow = m, ncol = m)
  for (i in 1:nrow(clust.edge.list)) {
    r <- clust.edge.list[i,]
    dist.mst[r$from, r$to] <- r$dist
    dist.mst[r$to, r$from] <- r$dist
  }
  dist.mst
}
gen.child.list.mst <- function(clust.edge.list, m) {
  child.list.mst <- vector("list", m)
  for (i in 1:m) {
    child.list.mst[[m]] <- vector()
  }
  for (i in 1:nrow(clust.edge.list)) {
    r <- clust.edge.list[i,]
    child.list.mst[[r$from]] <- c(child.list.mst[[r$from]] , r$to)
    child.list.mst[[r$to]] <- c(child.list.mst[[r$to]] , r$from)
  }
  child.list.mst
}

mst.cluster <- function(child.list.mst, m) {
  visited <<- rep(FALSE, m)
  clust.mst <<- rep(0, m)
  dfs <- function(u, clust) {
    if (visited[u]) {
      return()
    } else {
      visited[u] <<- TRUE
      clust.mst[u] <<- clust
      for (v in child.list.mst[[u]]) {
        dfs(v, clust)
      }
    }
  }
  for (clust in 1:k) {
    dfs.started <- FALSE
    for (u in 1:m) {
      if (!visited[u]) {
        dfs.started <- TRUE
        dfs(u, clust)
      }
      if (dfs.started) {
        break
      }
    }
  }
  clust.mst
}

gen.palette <- function(mst.clust, m) {
  palette <- c("red", "green", "blue", "magenta", "black", "grey")
  colors <- c()
  for (i in 1:m) {
    colors <- c(colors, palette[mst.clust[i]])
  }
  colors
}

seeds.dist.mst <- gen.dist.mst(seeds.clust.mst.edge.list, m)
seeds.child.list.mst <- gen.child.list.mst(seeds.clust.mst.edge.list, m)
seeds.mst.clust <- mst.cluster(seeds.child.list.mst, m)
seeds.mst.colors <- gen.palette(seeds.mst.clust, m)

```
We now plot the resulting MST.

```{r}
ggnet2(seeds.dist.mst, node.size = 3, node.color = seeds.mst.colors, directed=FALSE)
```

We now calculate the median point for each cluster as $\arg\min\limits_{j \in C} \sum\limits_{i \in C} d_{ij}$.

```{r}
get.mst.median.nodes <- function(clust.mst, orig.dist, k) {
  dist.clust.mst <- vector("numeric", k)
  median.node <- vector("numeric", k)
  for (cur.clust in 1:k) {
    nodes.in.clust <- which(clust.mst == cur.clust)
    min.sum.dist <- sum(orig.dist)
    for (u in nodes.in.clust) {
      sum.dist <- sum(orig.dist[u,nodes.in.clust])
      if (sum.dist < min.sum.dist) {
        median.node[cur.clust] <- u
        min.sum.dist <- sum.dist
        dist.clust.mst[cur.clust] <- min.sum.dist
      }
    }
  }
  median.node
}
seeds.mst.median.nodes <- get.mst.median.nodes(seeds.mst.clust, seeds.dist, k)
```

We now print the value of the objective function as calculated with the MST.

```{r}
get.mst.median.nodes.dist <- function(mst.median.nodes, mst.clust, orig.dist) {
  mst.median.nodes.dist <- rep(0, length(mst.median.nodes))
  for (i in 1:nrow(orig.dist)) {
    cur.cluster <- mst.clust[i]
    cur.median <- mst.median.nodes[cur.cluster]
    mst.median.nodes.dist[cur.cluster] <- mst.median.nodes.dist[cur.cluster] + orig.dist[i, cur.median]
  }
  mst.median.nodes.dist
}
seeds.mst.median.nodes.dist <- get.mst.median.nodes.dist(seeds.mst.median.nodes, seeds.mst.clust, seeds.dist)
cat("MST Objective:", sum(seeds.mst.median.nodes.dist), "\n")
```

We can see that this value is slightly more than double what is obtained by solving the optimization problem with AMPL.

Finally, we print the confusion matrix of the clusters obtained with the MST.

```{r}
seeds.predicted.clust.mst <- as.data.frame(clust.mst)

true.clust <- seeds[,8]
true.clust <- as.factor(true.clust)
levels(true.clust) <- true.clust[seeds.mst.median.nodes]

print(confusionMatrix(as.factor(seeds.predicted.clust.mst[,1]), true.clust))
```

# Clustering on a dataset with better linear separability

We test AMPL-based and MST-based clustering on a linearly separable dataset, generated with a Gaussian blob generator. 

```{r}
blobs <- read.table('blobs.txt')
blobs.clean <- blobs[,-3]
blobs.dist <- as.matrix(dist(blobs.clean))
write.table(blobs.dist, file='blobs_dist.dat', row.names = FALSE, col.names = FALSE)
plot(blobs[, 1:2])
```

We see that, this time, clusters are very clearly linearly separable.

The output of running clustering with AMPL is:
```
CPLEX 20.1.0.0: optimal integer solution; objective 238.2922866
5225 MIP simplex iterations
0 branch-and-bound nodes
```

```{r}
blobs.out <- read.table('blobs_out.dat')
blobs.ampl.cluster.medians <- get.cluster.medians(blobs.out)
cat("Cluster medians are: ", blobs.ampl.cluster.medians, "\n")
print(blobs[blobs.ampl.cluster.medians,])
k <- 4
blobs.confusion.matrix <- get.confusion.matrix(as.factor(blobs[,3]), blobs.out, k)
print(blobs.confusion.matrix)
dx <- blobs.out * blobs.dist
cat("sum(dij * xij) = ", sum(dx))
blobs.edge.list <- gen.edge.list(blobs.clean)
m <- nrow(blobs.dist)
blobs.mst.edge.list <- kruskal(blobs.edge.list, m)
k <- 4
n.edges <- nrow(blobs.mst.edge.list)
blobs.clust.mst.edge.list <- blobs.mst.edge.list[1:(n.edges - (k - 1)),]
blobs.dist.mst <- gen.dist.mst(blobs.clust.mst.edge.list, m)
blobs.child.list.mst <- gen.child.list.mst(blobs.clust.mst.edge.list, m)
blobs.mst.clust <- mst.cluster(blobs.child.list.mst, m)
blobs.mst.colors <- gen.palette(blobs.mst.clust, m)
ggnet2(blobs.dist.mst, node.size = 3, node.color = blobs.mst.colors, directed=FALSE)
blobs.mst.median.nodes <- get.mst.median.nodes(blobs.mst.clust, blobs.dist, k)
blobs.mst.median.nodes.dist <- get.mst.median.nodes.dist(blobs.mst.median.nodes, blobs.mst.clust, blobs.dist)
cat("MST Objective:", sum(blobs.mst.median.nodes.dist), "\n")
blobs.predicted.clust.mst <- as.data.frame(clust.mst)

true.clust <- blobs[,3]
true.clust <- as.factor(true.clust)
levels(true.clust) <- true.clust[blobs.mst.median.nodes]

print(confusionMatrix(as.factor(blobs.predicted.clust.mst[,1]), true.clust))
```
This time, the confusion matrix shows that MST-based clustering is able to achieve perfect accuracy, as is achieved by AMPL, and the two values of the objective function are the same.

# Conclusions

We can see that for a dataset with poor linear separability, solving the problem using the minimum spanning tree as heuristic provides for a very poor quality clustering, with a value of the objective function being more than double the one obtained using AMPL and accuracy that is barely better than random guessing.

# References
